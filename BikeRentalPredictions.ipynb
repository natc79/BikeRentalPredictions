{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Demand for Rental Bikes\n",
    "\n",
    "This undertakes linear regression, decision tree analysis, and random forest analysis to predict the number of bike rentals that occur in a given day/hour/month.  The data comes from bike rentals in Washington DC over the time period 2011 to 2012.  This data was compiled Hadi Fanaee-T at the University of Porto.  The file contains 17380 rows, with each row representing the number of bike rentals for a single hour of a single day. The data also includes variables on weather that can affect bike rentals.  The data can be obtained from the University of California, Irvine's website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
      "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
      "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
      "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
      "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
      "4        5  2011-01-01       1   0     1   4        0        6           0   \n",
      "\n",
      "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
      "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
      "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
      "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
      "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
      "4           1  0.24  0.2879  0.75        0.0       0           1    1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bike_rentals = pd.read_csv(\"bike_rental_hour.csv\")\n",
    "print(bike_rentals.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20e21cc89e8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD8CAYAAAB6paOMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20e1e767198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "bike_rentals[\"cnt\"].hist(edgecolor=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations between the different RHS variables and LHS.\n",
    "\n",
    "Examining the correlations between the different variables we observed that instant, season, year, month are all highly correlated.  This suggests that we should not include this variable with season, month, year.  weathersit is also highly correlated with humidity; casual is highly correlated with temperature and humidity.  Atemp and temp are highly correlated.  The variance inflation factor (VIF = 1/(1-R^2)) over 2.50 (or R^2 = 0.60) provides an indication of potential multicollinearity.  Multicollinearity is not an issue if we are only concerned with prediction and not interpretation of the variable of interest estimates.  It is also not a concern when it is simply a factor (non-linear) to other variables included in the model or categorical dummy variables.\n",
    "\n",
    "Registered plus causal equal cnt and therefore are alternative outcome variable to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instant       0.278379\n",
      "season        0.178056\n",
      "yr            0.250495\n",
      "mnth          0.120638\n",
      "hr            0.394071\n",
      "holiday      -0.030927\n",
      "weekday       0.026900\n",
      "workingday    0.030284\n",
      "weathersit   -0.142426\n",
      "temp          0.404772\n",
      "atemp         0.400929\n",
      "hum          -0.322911\n",
      "windspeed     0.093234\n",
      "casual        0.694564\n",
      "registered    0.972151\n",
      "cnt           1.000000\n",
      "Name: cnt, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#examine how various variables are correlated with cnt\n",
    "corrbike = bike_rentals.corr()\n",
    "print(corrbike[\"cnt\"])\n",
    "#print(corrbike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
      "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
      "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
      "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
      "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
      "4        5  2011-01-01       1   0     1   4        0        6           0   \n",
      "5        6  2011-01-01       1   0     1   5        0        6           0   \n",
      "6        7  2011-01-01       1   0     1   6        0        6           0   \n",
      "7        8  2011-01-01       1   0     1   7        0        6           0   \n",
      "8        9  2011-01-01       1   0     1   8        0        6           0   \n",
      "9       10  2011-01-01       1   0     1   9        0        6           0   \n",
      "\n",
      "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \\\n",
      "0           1  0.24  0.2879  0.81     0.0000       3          13   16   \n",
      "1           1  0.22  0.2727  0.80     0.0000       8          32   40   \n",
      "2           1  0.22  0.2727  0.80     0.0000       5          27   32   \n",
      "3           1  0.24  0.2879  0.75     0.0000       3          10   13   \n",
      "4           1  0.24  0.2879  0.75     0.0000       0           1    1   \n",
      "5           2  0.24  0.2576  0.75     0.0896       0           1    1   \n",
      "6           1  0.22  0.2727  0.80     0.0000       2           0    2   \n",
      "7           1  0.20  0.2576  0.86     0.0000       1           2    3   \n",
      "8           1  0.24  0.2879  0.75     0.0000       1           7    8   \n",
      "9           1  0.32  0.3485  0.76     0.0000       8           6   14   \n",
      "\n",
      "   time_label  \n",
      "0           4  \n",
      "1           4  \n",
      "2           4  \n",
      "3           4  \n",
      "4           4  \n",
      "5           4  \n",
      "6           4  \n",
      "7           1  \n",
      "8           1  \n",
      "9           1  \n"
     ]
    }
   ],
   "source": [
    "# place the hours into different time periods\n",
    "def assign_label(hr):\n",
    "    if 6 < hr and hr <= 12:\n",
    "        return 1\n",
    "    elif 12 < hr and hr <= 18:\n",
    "        return 2\n",
    "    elif 18 < hr and hr <= 24:\n",
    "        return 3\n",
    "    elif 0 <= hr and hr <= 6:\n",
    "        return 4\n",
    "    \n",
    "bike_rentals[\"time_label\"]=bike_rentals.apply(lambda x: assign_label(x[\"hr\"]), axis=1)\n",
    "print(bike_rentals.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chosing an error metric\n",
    "\n",
    "Mean squared error is chosen because it follows from standard linear regressions.  Since the variable bike count can be considered continous it makes sense to choose this value.  Moreover this statistic tends to weigh values further away from the norm more significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#shuffle the data to make sure we are drawing random subset\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "randind = np.random.permutation(len(bike_rentals))\n",
    "shuffled_bike_rentals=bike_rentals.loc[randind]\n",
    "train = shuffled_bike_rentals.iloc[0:math.floor(len(bike_rentals)*0.80)]\n",
    "\n",
    "#select any rows in bike_rentals that aren't in train to be in testing set\n",
    "test = shuffled_bike_rentals.loc[~bike_rentals.index.isin(train.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train: 19367.21\n",
      "MSE test:  19265.02\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#grab columns that have a high correlation, but not so high that it is nearly perfect\n",
    "#we also only include columns that are not too highly correlated with each other due to reasons of multicollinearity\n",
    "cols = [\"weathersit\", \"temp\", \"hum\", \"weekday\", \"workingday\"]\n",
    "\n",
    "#create various dummy variables\n",
    "bike_rentals = pd.concat([bike_rentals, pd.get_dummies(bike_rentals['yr'], prefix=\"yr\")], axis=1)\n",
    "bike_rentals = pd.concat([bike_rentals, pd.get_dummies(bike_rentals['season'], prefix=\"season\")], axis=1)\n",
    "bike_rentals = pd.concat([bike_rentals, pd.get_dummies(bike_rentals['mnth'], prefix=\"mt\")], axis=1)\n",
    "bike_rentals = pd.concat([bike_rentals, pd.get_dummies(bike_rentals['time_label'], prefix=\"time\")], axis=1)\n",
    "\n",
    "#This grabs the variables with year and puts into a label name for the regression\n",
    "colnames = train.columns\n",
    "for c in colnames:\n",
    "    if c.startswith(\"yr_\",0) | c.startswith(\"mt_\",0) | c.startswith(\"season_\",0) | c.startswith(\"time_\",0):\n",
    "        cols.append(c)\n",
    "#print(cols)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train[cols],train[\"cnt\"])\n",
    "pr_y = lr.predict(train[cols])\n",
    "mse_train = mean_squared_error(train[\"cnt\"],pr_y)\n",
    "pr_y = lr.predict(test[cols])\n",
    "mse_test = mean_squared_error(test[\"cnt\"],pr_y)\n",
    "print(\"MSE train: {}\".format(round(mse_train,2)))\n",
    "print(\"MSE test:  {}\".format(round(mse_test,2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction results\n",
    "\n",
    "The predictions give a very high mean squared error indicating that the fit is not very good between predictions and actual values.  At the same time the difference in MSE between train and test models is not large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train: 6591.86\n",
      "MSE test:  7717.76\n",
      "MSE train: 6277.2\n",
      "MSE test:  7709.6\n"
     ]
    }
   ],
   "source": [
    "#applying the decision tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "cols = [\"weathersit\", \"temp\", \"hum\", \"weekday\", \"workingday\", \"yr\", \"season\", \"mnth\", \"time_label\"]\n",
    "dt = DecisionTreeRegressor(criterion='mse', splitter='best', max_depth=None, min_samples_split=15, min_samples_leaf=2)\n",
    "dt.fit(train[cols], train[\"cnt\"])\n",
    "pr_train = dt.predict(train[cols])\n",
    "mse_train = mean_squared_error(train[\"cnt\"],pr_train)\n",
    "pr_test = dt.predict(test[cols])\n",
    "mse_test = mean_squared_error(test[\"cnt\"],pr_test)\n",
    "print(\"MSE train: {}\".format(round(mse_train,2)))\n",
    "print(\"MSE test:  {}\".format(round(mse_test,2)))\n",
    "\n",
    "\n",
    "dt = DecisionTreeRegressor(criterion='mse', splitter='best', max_depth=15, min_samples_split=8, min_samples_leaf=2)\n",
    "dt.fit(train[cols], train[\"cnt\"])\n",
    "pr_train = dt.predict(train[cols])\n",
    "mse_train = mean_squared_error(train[\"cnt\"],pr_train)\n",
    "pr_test = dt.predict(test[cols])\n",
    "mse_test = mean_squared_error(test[\"cnt\"],pr_test)\n",
    "print(\"MSE train: {}\".format(round(mse_train,2)))\n",
    "print(\"MSE test:  {}\".format(round(mse_test,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree analysis results\n",
    "\n",
    "The decision tree analysis significantly improves the mean squared error cutting it down\n",
    "by more than half of the error of the linear regression.  The mean-squared error, however, still remains high.  The benefit of the decision tree is that it executes fast and can handle non-linearities in the data that are not possible to be handled by a linear regression.  However, its limitations is that it has a tendency to overfit (i.e. introduce increased error into the test model set), but this can be somewhat corrected by limiting the depth, sample split, and leaves in the tree.  Also overfitting tends to arise when variable have multicollinearity as the decision tree will choose the best one that may be good for the train model, but not for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train: 11714.01\n",
      "MSE test:  11584.0\n"
     ]
    }
   ],
   "source": [
    "#Applying a random forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=15, criterion='mse', max_depth=7, min_samples_split=6, min_samples_leaf=1)\n",
    "rf.fit(train[cols], train[\"cnt\"])\n",
    "pr_train = rf.predict(train[cols])\n",
    "mse_train = mean_squared_error(train[\"cnt\"],pr_train)\n",
    "pr_test = rf.predict(test[cols])\n",
    "mse_test = mean_squared_error(test[\"cnt\"],pr_test)\n",
    "print(\"MSE train: {}\".format(round(mse_train,2)))\n",
    "print(\"MSE test:  {}\".format(round(mse_test,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train: 5883.1\n",
      "MSE test:  6678.07\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=15, criterion='mse', max_depth=None, min_samples_split=10, min_samples_leaf=2)\n",
    "rf.fit(train[cols], train[\"cnt\"])\n",
    "pr_train = rf.predict(train[cols])\n",
    "mse_train = mean_squared_error(train[\"cnt\"],pr_train)\n",
    "pr_test = rf.predict(test[cols])\n",
    "mse_test = mean_squared_error(test[\"cnt\"],pr_test)\n",
    "print(\"MSE train: {}\".format(round(mse_train,2)))\n",
    "print(\"MSE test:  {}\".format(round(mse_test,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Results\n",
    "\n",
    "The random forest regressor is a slight improvement over the decision tree analysis model.  However, the mean-squared-error still is high.  The benefits of the random forest comes through its accuracy.  Nevertheless, the random model is more difficult to interpret as it hard to tell what parameters are driving the estimates and takes longer to execute compared to linear regression or decision tree models.\n",
    "\n",
    "### Extensions:  Creating a Weather Index\n",
    "\n",
    "The combined weather could have have a non-linear effect in regression estimates.  Therefore combining the three variables into single index could add additional predictive power to the regression algorithms.  Below an index based on principal component analysis is used to repeat the regression exercises above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "#here we just the principal component\n",
    "pca = PCA(n_components=3)\n",
    "#now we need to scale the values before using in the PCA\n",
    "pcavars = scale(bike_rentals[[\"weathersit\", \"temp\", \"hum\"]])\n",
    "pca.fit(pcavars)\n",
    "#The amount of variance that each PC explains\n",
    "var= pca.explained_variance_ratio_\n",
    "#examine how much is explained by the various components\n",
    "var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "#print(var1)\n",
    "\n",
    "#from the cumulative sum of the variance we see that the first two explain about 80%\n",
    "#for ease of analysis we will currently only use 1 PCA.\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(pcavars)\n",
    "#bike_rentals[\"TempI1\"] = pca.fit_transform(pcavars)\n",
    "pca_comp = pca.fit_transform(pcavars)\n",
    "#print(pca_comp)\n",
    "#transform variables into ones that we can use in the regression\n",
    "pca_c1 = []\n",
    "for comp in pca_comp:\n",
    "    for item in comp:\n",
    "        pca_c1.append(item)\n",
    "bike_rentals[\"WeatherI1\"] = pca_c1\n",
    "\n",
    "#get only the first principal component (column)\n",
    "#print(pca.fit_transform(bike_rentals[[\"weathersit\", \"temp\", \"hum\"]]))\n",
    "#print(pca.get_params())\n",
    "#fact = FactorAnalysis()\n",
    "#fact.fit(bike_rentals[\"weathersit\", \"temp\", \"hum\"])\n",
    "#fact.fit_transform(bike_rentals[\"weathersit\", \"temp\", \"hum\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train: 14771.27\n",
      "MSE test:  15124.68\n"
     ]
    }
   ],
   "source": [
    "randind = np.random.permutation(len(bike_rentals))\n",
    "shuffled_bike_rentals=bike_rentals.loc[randind]\n",
    "train = shuffled_bike_rentals.iloc[0:math.floor(len(bike_rentals)*0.80)]\n",
    "\n",
    "#select any rows in bike_rentals that aren't in train to be in testing set\n",
    "test = shuffled_bike_rentals.loc[~bike_rentals.index.isin(train.index)]\n",
    "\n",
    "cols = [\"WeatherI1\",\"weathersit\", \"temp\", \"hum\",\"weekday\", \"workingday\"]\n",
    "colnames = train.columns\n",
    "for c in colnames:\n",
    "    if c.startswith(\"yr_\",0) | c.startswith(\"mt_\",0) | c.startswith(\"season_\",0) | c.startswith(\"time_\",0):\n",
    "        cols.append(c)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train[cols],train[\"cnt\"])\n",
    "pr_y = lr.predict(train[cols])\n",
    "mse_train = mean_squared_error(train[\"cnt\"],pr_y)\n",
    "pr_y = lr.predict(test[cols])\n",
    "mse_test = mean_squared_error(test[\"cnt\"],pr_y)\n",
    "print(\"MSE train: {}\".format(round(mse_train,2)))\n",
    "print(\"MSE test:  {}\".format(round(mse_test,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train: 5908.36\n",
      "MSE test:  7415.36\n"
     ]
    }
   ],
   "source": [
    "cols = [\"WeatherI1\",\"weathersit\", \"temp\", \"hum\", \"weekday\", \"workingday\", \"yr\", \"season\", \"mnth\", \"time_label\"]\n",
    "dt = DecisionTreeRegressor(criterion='mse', splitter='best', max_depth=15, min_samples_split=8, min_samples_leaf=2)\n",
    "dt.fit(train[cols], train[\"cnt\"])\n",
    "pr_train = dt.predict(train[cols])\n",
    "mse_train = mean_squared_error(train[\"cnt\"],pr_train)\n",
    "pr_test = dt.predict(test[cols])\n",
    "mse_test = mean_squared_error(test[\"cnt\"],pr_test)\n",
    "print(\"MSE train: {}\".format(round(mse_train,2)))\n",
    "print(\"MSE test:  {}\".format(round(mse_test,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train: 5462.01\n",
      "MSE test:  6383.3\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=15, criterion='mse', max_depth=None, min_samples_split=10, min_samples_leaf=2)\n",
    "rf.fit(train[cols], train[\"cnt\"])\n",
    "pr_train = rf.predict(train[cols])\n",
    "mse_train = mean_squared_error(train[\"cnt\"],pr_train)\n",
    "pr_test = rf.predict(test[cols])\n",
    "mse_test = mean_squared_error(test[\"cnt\"],pr_test)\n",
    "print(\"MSE train: {}\".format(round(mse_train,2)))\n",
    "print(\"MSE test:  {}\".format(round(mse_test,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions \n",
    "\n",
    "The additional index developed from PCA improves the estimates for the linear regression, but introduces additional error into into the test model for decision tree estimates.  There is also marginal improvements in the error for the MSE test model using a random forest estimator.  In general, if we care about accuracy the random forest provides the best model, however, for speed and interpretation the decision tree regressor may be a balanced option that provides significantly more accuracy over a linear regression and better performance speed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
